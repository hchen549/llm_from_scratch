[project]
name = "llm-from-scratch"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12,<3.13"
dependencies = [
    "debugpy>=1.8.14",
    "medusa-llm>=0.1",
    "llm-basics"
]


[dependency-groups]
gpu = [
  "accelerate>=1.2.1,<2.0.0",
  "click>=8.1.8,<9.0.0",
  "datasets>=3.2.0,<4.0.0",
  "gcsfs~=2024.2.0",
  "google-cloud-storage>=2.18.0,<3.0.0",
  "langchain>=0.2.17",
  "langchain-openai>=0.1.25",
  "peft>=0.14.0,<0.15.0",
  "transformers>=4.48.0,<5.0.0",
  "bitsandbytes>=0.45.1",
  "trl>=0.13.0,<0.14.0",
  "torch~=2.6.0; platform_system == 'Linux'",
  "torchvision~=0.21.0; platform_system == 'Linux'",
  "flash-attn==2.7.4.post1; platform_system == 'Linux' and platform_machine == 'x86_64'",
]

[tool.uv.sources]
llm-basics = { path = "./llm_basics", editable = true }
torch = { index = "pytorch-cu124" }
torchvision = { index = "pytorch-cu124" }


[[tool.uv.index]]
name = "pytorch-cu124"
url = "https://download.pytorch.org/whl/cu124"
explicit = true

[tool.uv]
no-build-isolation-package = ["flash-attn"]