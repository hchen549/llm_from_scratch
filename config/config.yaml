
# Model configuration (parameters used in BasicsTransformerLM)
model:
  vocab_size: 100000
  context_length: 256
  d_model: 768
  num_layers: 12
  num_heads: 12
  d_ff: 3072
  rope_theta: 10000.0

# Benchmark configuration
benchmark:
  batch_size: 48
  forward_and_backward_pass: false
  warmup_iterations: 50
  num_iterations: 100
  variable_length: false

# Optimizer configuration
optimizer:
  learning_rate: 0.0001
  foreach: true

# Profiler configuration
profiler:
  enable_profiler: false
  profiler_output_dir: "./profiler_traces"
  profiler_trace_file: "trace.json"
  memory_timeline_file: "memory_timeline.html"

# System configuration
system:
  device: "cuda"
  random_seed: 42

# Triton configuration
triton:
  rms: false